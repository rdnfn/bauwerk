{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results 1: SolarBatteryHouse\n",
    "\n",
    "In this we run some experiments on the default version of the main Bauwerk environment, `SolarBatteryHouse`. For a full description of the environment, [see here](../../envs/solar_battery_house.ipynb). We recommend rerunning these experiments as baselines for work using this environment.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "This notebook uses Gym v0.21 because this is required for the Stable Baselines3 (SB3). Thus, the environment API is different compared to other pages in the Bauwerk docs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "There is no one way that performance on the `SolarBatteryHouse` can or should be measured. One obvious choice would be to directly compare the cumulative reward, i.e. overall grid energy payments. However, the magnitude of this value completely depends on the configuration of the environment. Thus, this can make it difficult to compare performance between environments, even within Bauwerk. Instead, we propose using the access to the optimal control actions by considering *performance relative to random and optimal control*,\n",
    "\n",
    "$p' = \\frac{p_m-p_r}{p_o-p_r}$,\n",
    "\n",
    "where $p_m$ is the average reward of the method to be evaluated, $p_r$ and $p_o$ the average rewards of random and optimal control respectively. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "With the performance measure $p'$, a value $>0$ means that the method is better than random, a value close to $1$ means that the method is close to optimal, a value $<0$ means that the method is worse than random, i.e. completely useless.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "### Random actions\n",
    "\n",
    "Let's get started by establishing the lowest bar: what's the performance we get, when we just take random actions in the environment (sampled from the action space). To do this, we first need to determine how many actions in the environment we want to evaluate over (`EVAL_LEN`) and setup some helper code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Setup and helper code\n",
    "import bauwerk\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "EVAL_LEN = 24*30 # evaluate on 1 month of actions\n",
    "\n",
    "# Create SolarBatteryHouse environment\n",
    "env = gym.make(\"bauwerk/SolarBatteryHouse-v0\")\n",
    "\n",
    "def evaluate_actions(actions, env):\n",
    "    cum_reward = 0\n",
    "    obs = env.reset()\n",
    "    for action in actions:\n",
    "        obs, reward, done, info = env.step(np.array(action, dtype=np.float32))\n",
    "        cum_reward += reward\n",
    "    \n",
    "    return cum_reward / len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward with random actions: -0.5459 (standard deviation: 0.0233)\n"
     ]
    }
   ],
   "source": [
    "# mean random performance over 100 trials\n",
    "random_trials = [evaluate_actions([env.action_space.sample() for _ in range(EVAL_LEN)], env) for _ in range(100)]\n",
    "random_std = np.std(random_trials)\n",
    "p_rand = np.mean(random_trials)\n",
    "# note: std here is between different trials (of multiple actions)\n",
    "print(f\"Avg reward with random actions: {p_rand:.4f} (standard deviation: {random_std:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal actions\n",
    "\n",
    "`SolarBatteryHouse` is a fully tractable environment. Thus, Bauwerk can easily compute the theoretically optimal actions one can take in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward (per step) with optimal actions: -0.1036\n"
     ]
    }
   ],
   "source": [
    "optimal_actions, _ = bauwerk.solve(env)\n",
    "p_opt = evaluate_actions(optimal_actions.reshape((-1,1))[:EVAL_LEN], env)\n",
    "print(f\"Avg reward (per step) with optimal actions: {p_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning agent\n",
    "\n",
    "Next we consider a simple reinforcement learning (RL) agent. We use [Stable Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) to access RL algorithm implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0xffff5b79f970>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=\"bauwerk/SolarBatteryHouse-v0\", \n",
    "    verbose=0,\n",
    ")\n",
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward (per step) with model actions: -0.1676\n"
     ]
    }
   ],
   "source": [
    "# Obtaining model actions and evaluating them\n",
    "model_actions = []\n",
    "obs = env.reset()\n",
    "for i in range(EVAL_LEN):\n",
    "    action, _states = model.predict(obs)\n",
    "    model_actions.append(action)\n",
    "    obs, _, _, _ = env.step(action)\n",
    "\n",
    "p_model = evaluate_actions(model_actions[:EVAL_LEN], env)\n",
    "print(f\"Avg reward (per step) with model actions: {p_model:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance relative to random and optimal: 0.8553\n"
     ]
    }
   ],
   "source": [
    "# Measuring performance relative to random and optimal\n",
    "p_bar = (p_model - p_rand)/(p_opt - p_rand)\n",
    "print(f\"Performance relative to random and optimal: {p_bar:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
