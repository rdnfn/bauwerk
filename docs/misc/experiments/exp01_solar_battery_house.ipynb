{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results 1: SolarBatteryHouse\n",
    "\n",
    "To start of we run some experiments on the default version of the main Bauwerk environment, `SolarBatteryHouse`. We recommend rerunning these experiments as baselines for work using this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "There is no one way that performance on the `SolarBatteryHouse` can or should be measured. One obvious choice would be to directly compare the cumulative reward, i.e. overall grid energy payments. However, the magnitude of this value completely depends on the configuration of the environment. Thus, this can make it difficult to compare performance between environments, even within Bauwerk. Instead, we propose using the access to the optimal control actions by considering *performance relative to random and optimal control*,\n",
    "\n",
    "$p' = \\frac{p_m-p_r}{p_o-p_r}$,\n",
    "\n",
    "where $p_m$ is the average reward of the method to be evaluated, $p_r$ and $p_o$ the average rewards of random and optimal control respectively. A value $>0$ means that the method is better than random, a value close to $1$ means that the method is close to optimal, a value $<0$ means that the method is worse than random, i.e. completely useless.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "### Random actions\n",
    "\n",
    "With this let's get started by establishing the lowest bar: what's the performance we get when we just take random actions in the environment (sampled from the action space). To do this we first need to determine how many actions in the environment we want to evaluate over (`EVAL_LEN`) and setup some helper code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and helper code\n",
    "import bauwerk\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "EVAL_LEN = 24*30 # evaluate on 1 month of actions\n",
    "\n",
    "# Create SolarBatteryHouse environment\n",
    "env = gym.make(\"bauwerk/SolarBatteryHouse-v0\")\n",
    "\n",
    "def evaluate_actions(actions, env):\n",
    "    cum_reward = 0\n",
    "    obs = env.reset()\n",
    "    for action in actions:\n",
    "        obs, reward, done, info = env.step(np.array(action, dtype=np.float32))\n",
    "        cum_reward += reward\n",
    "    \n",
    "    return cum_reward / len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward with random actions: -0.5455792138136959 (standard deviation: 0.025515726632037904)\n"
     ]
    }
   ],
   "source": [
    "# mean random performance over 100 trials\n",
    "random_trials = [evaluate_actions([env.action_space.sample() for _ in range(EVAL_LEN)], env) for _ in range(100)]\n",
    "random_std = np.std(random_trials)\n",
    "p_rand = np.mean(random_trials)\n",
    "# note: std here is between different trials (of multiple actions)\n",
    "print(f\"Avg reward with random actions: {p_rand} (standard deviation: {random_std})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal actions\n",
    "\n",
    "`SolarBatteryHouse` is a fully tractable environment, thus Bauwerk can easily compute the theoretically optimal actions one can take in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward (per step) with optimal actions: -0.10358608226144295\n"
     ]
    }
   ],
   "source": [
    "optimal_actions, _ = bauwerk.solve(env)\n",
    "p_opt = evaluate_actions(optimal_actions.reshape((-1,1))[:EVAL_LEN], env)\n",
    "print(f\"Avg reward (per step) with optimal actions: {p_opt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning agent\n",
    "\n",
    "Next we consider a simple reinforcement learning (RL) agent. We use [Stable Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) to access RL algorithm implementations.\n",
    "\n",
    "Coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
