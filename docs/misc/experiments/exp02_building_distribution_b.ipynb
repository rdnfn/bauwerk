{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results 2: Building Distribution B\n",
    "\n",
    "After running some initial experiments on a single environment, let us consider multiple buildings at the same time.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "In this notebook Gym v0.21 is used, because this version is required for Stable Baselines3 (SB3), an RL algorithm library used here. Thus, the environment API is different compared to other pages in the Bauwerk docs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As in previous experiments we consider the *performance relative to random and optimal control*,\n",
    "\n",
    "$p' = \\frac{p_m-p_r}{p_o-p_r}$,\n",
    "\n",
    "where $p_m$ is the average reward of the method to be evaluated, $p_r$ and $p_o$ the average rewards of random and optimal control respectively. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "With the performance measure $p'$, a value $>0$ means that the method is better than random, a value close to $1$ means that the method is close to optimal, a value $<0$ means that the method is worse than random, i.e. completely useless.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up distribution\n",
    "\n",
    "To start off we import all relevant packages and sample tasks from distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and helper code\n",
    "import bauwerk\n",
    "import bauwerk.evaluation\n",
    "import bauwerk.benchmarks\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# The length of each task, i.e. how long we aim to \n",
    "TASK_LEN = 24*30 # evaluate on 1 month of actions\n",
    "\n",
    "# Create SolarBatteryHouse environment\n",
    "build_dist_b = bauwerk.benchmarks.BuildDistB(seed=100, task_ep_len=TASK_LEN)\n",
    "print(f\"Sampled {len(build_dist_b.train_tasks)} training and\"\n",
    "    f\" {len(build_dist_b.test_tasks)} test tasks from building distribution B.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "### Random, optimal & *nocharge* performance\n",
    "\n",
    "To get started, we compute the performance of taking random, optimal and no charging actions in\n",
    "each environment. We can see that, with random actions, a larger battery size leads to worse performance as the battery size allows for more (expensive) charging from the grid. On the other hand, for optimal actions, increasing battery size leads to increasing performance as it provides additional flexibility. This effect plateaus eventually as larger battery sizes and additional flexibility can no longer be used within the system. As expected, when not taking any charging actions there is no effect of battery size on performance. Note that here we only consider *operational performance* but ignore other costs such as acquiring and installing a larger battery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data on different training tasks\n",
    "env = build_dist_b.make_env()\n",
    "batt_sizes = []\n",
    "perf_opts = []\n",
    "perf_rands = []\n",
    "perf_nocharges = []\n",
    "for i, task in enumerate(build_dist_b.train_tasks):\n",
    "    env = build_dist_b.make_env()\n",
    "    env.set_task(task)\n",
    "    perf_opt = bauwerk.evaluation.get_optimal_perf(env, TASK_LEN=TASK_LEN)\n",
    "    perf_rand, perf_rand_std = bauwerk.evaluation.get_avg_rndm_perf(\n",
    "        env, \n",
    "        TASK_LEN=TASK_LEN,\n",
    "        num_samples=10,\n",
    "    )\n",
    "    perf_nocharge = bauwerk.evaluation.evaluate_actions(np.zeros((TASK_LEN,1)), env)\n",
    "    batt_sizes.append(env.cfg.battery_size)\n",
    "    perf_opts.append(perf_opt)\n",
    "    perf_rands.append(perf_rand)\n",
    "    perf_nocharges.append(perf_nocharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the effect of battery size in training task performance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(batt_sizes, perf_opts)\n",
    "plt.xlabel(\"Battery size (kWh)\")\n",
    "plt.title(\"Optimal actions\")\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Random actions\")\n",
    "plt.ylabel(\"Avg energy payment per step ($)\")\n",
    "plt.scatter(batt_sizes, perf_rands)\n",
    "plt.xlabel(\"Battery size (kWh)\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"No charging actions\")\n",
    "plt.scatter(batt_sizes, perf_nocharges)\n",
    "plt.xlabel(\"Battery size (kWh)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL agents\n",
    "\n",
    "Next we consider simple reinforcement learning (RL) agents. We use [Stable Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) to access RL algorithm implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for evaluating methods\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "def eval_model(model, env):\n",
    "    # Obtaining model actions and evaluating them\n",
    "    model_actions = []\n",
    "    obs = env.reset()\n",
    "    for i in range(TASK_LEN):\n",
    "        action, _states = model.predict(obs)\n",
    "        model_actions.append(action)\n",
    "        obs, _, _, _ = env.step(action)\n",
    "\n",
    "    p_model = bauwerk.evaluation.evaluate_actions(model_actions[:TASK_LEN], env)\n",
    "    return p_model\n",
    "\n",
    "# callback for evaluating callback during training\n",
    "class EvalCallback(BaseCallback):\n",
    "    def __init__(self, eval_freq = 24*7, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.data = []\n",
    "        self.eval_freq = eval_freq\n",
    "        self.eval_env = gym.make(\"bauwerk/SolarBatteryHouse-v0\")\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        self.data.append(eval_model(self.model, self.eval_env))\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.eval_freq == 0:\n",
    "            self.data.append(eval_model(self.model, self.eval_env))\n",
    "\n",
    "        return True\n",
    "\n",
    "# Measuring performance relative to random and optimal\n",
    "def compute_rel_perf(p_model, p_rand, p_opt):\n",
    "    return (p_model - p_rand)/(p_opt - p_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC\n",
    "\n",
    "After setting up all helper functions we consider a popular RL algorithm - *Soft Actor-Critic* (SAC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "model_sac = SAC(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=\"bauwerk/SolarBatteryHouse-v0\", \n",
    "    verbose=0,\n",
    ")\n",
    "sac_callback = EvalCallback()\n",
    "model_sac.learn(total_timesteps=NUM_TRAIN_STEP,callback=sac_callback)\n",
    "\n",
    "p_model_sac = eval_model(model_sac, env)\n",
    "\n",
    "print(f\"Avg reward (per step) with model actions: {p_model_sac:.4f}\")\n",
    "\n",
    "p_rel_sac = compute_rel_perf(p_model_sac)\n",
    "print(f\"Performance relative to random and optimal: {p_rel_sac:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same amount of training time on the environment (1 year), SAC notably outperforms PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results plot\n",
    "\n",
    "Next we plot the combined results of the baselines, random and optimal actions, and our RL agents, PPO and SAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0,NUM_TRAIN_STEP,ppo_callback.eval_freq)\n",
    "plt.plot(\n",
    "    x, ppo_callback.data[:NUM_TRAIN_STEP//ppo_callback.eval_freq + 1], label=\"PPO\"\n",
    ")\n",
    "plt.plot(\n",
    "    x, sac_callback.data[:NUM_TRAIN_STEP//ppo_callback.eval_freq + 1], label=\"SAC\"\n",
    ")\n",
    "plt.hlines(p_opt, 0, NUM_TRAIN_STEP, label=\"Optimal\", linestyle=\":\", color=\"black\")\n",
    "plt.hlines(p_rand, 0, NUM_TRAIN_STEP, label=\"Random\", linestyle=\"--\", color=\"grey\")\n",
    "plt.hlines(p_nocharge, 0, NUM_TRAIN_STEP, label=\"No charging\", linestyle=\"-.\", color=\"lightblue\")\n",
    "plt.legend()\n",
    "plt.ylabel(f\"avg grid payment (per timestep)\")\n",
    "plt.xlabel(f\"timesteps (each {env.cfg.time_step_len}h)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note for reproducibility: this entire notebook was executed inside the devcontainer of the Bauwerk repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
